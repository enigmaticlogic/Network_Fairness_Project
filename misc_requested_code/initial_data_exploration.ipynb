{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: \n",
      "Type: Graph\n",
      "Number of nodes: 4039\n",
      "Number of edges: 88234\n",
      "Average degree:  43.6910\n",
      "{'0': {}, '48': {}, '53': {}, '54': {}, '73': {}, '88': {}, '92': {}, '119': {}, '126': {}, '133': {}, '194': {}, '236': {}, '280': {}, '299': {}, '315': {}, '322': {}, '346': {}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\17346\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\17346\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:469: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Pearson correlation is:  0.947631387699498\n"
     ]
    }
   ],
   "source": [
    "import networkx as nx\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from scipy import stats\n",
    "import os\n",
    "\n",
    "def read_nodes(node_file, num_feas):\n",
    "\n",
    "    '''\n",
    "    Returns a list of tuples (node name, dict of features for that node)\n",
    "    '''\n",
    "    \n",
    "    featname_file = node_file.replace('.feat', '.featnames')\n",
    "\n",
    "    with open(node_file) as opened_node:\n",
    "        node_file_lines = opened_node.readlines()\n",
    "    \n",
    "    with open(featname_file) as opened_featname:\n",
    "        featname_file_lines = opened_featname.readlines()\n",
    "\n",
    "    node_list = []\n",
    "    for line in node_file_lines:\n",
    "        split_line = line.split()\n",
    "        if len(split_line[1:]) != len(featname_file_lines):\n",
    "            print('error: features recorded not equal features expected')\n",
    "        node_name = split_line[0]\n",
    "        node_feas = -np.ones(num_feas)\n",
    "        for i in range(len(split_line[1:])):\n",
    "            split_featname_line = featname_file_lines[i].split()\n",
    "            fea_index = int(split_featname_line[3].strip())\n",
    "            node_feas[fea_index] = split_line[i + 1]\n",
    "        current_node = (node_name, dict(features=node_feas))\n",
    "        node_list.append(current_node)\n",
    "            \n",
    "    return node_list\n",
    "\n",
    "def read_ego_nodes(node_file, num_feas):\n",
    "\n",
    "    '''\n",
    "    Returns a list of tuples (node name=str, dict of features for that node)\n",
    "    '''\n",
    "    \n",
    "    featname_file = node_file.replace('.egofeat', '.featnames')\n",
    "    \n",
    "    with open(node_file) as opened_node:\n",
    "        node_file_lines = opened_node.readlines()\n",
    "        \n",
    "    with open(featname_file) as opened_featname:\n",
    "        featname_file_lines = opened_featname.readlines()\n",
    "\n",
    "    node_list = []\n",
    "    for line in node_file_lines:\n",
    "            split_line = line.split() \n",
    "            node_feas = -np.ones(num_feas)\n",
    "            node_name = ''\n",
    "            for s in node_file:\n",
    "                if s.isdigit():\n",
    "                    node_name += s\n",
    "            for i in range(len(split_line)):\n",
    "                split_featname_line = featname_file_lines[i].split()\n",
    "                fea_index = int(split_featname_line[3])\n",
    "                node_feas[fea_index] = split_line[i]\n",
    "            current_node = (node_name, dict(features=node_feas))\n",
    "            node_list.append(current_node)\n",
    "            \n",
    "    return node_list\n",
    "\n",
    "def read_edges(edge_file):\n",
    "\n",
    "    '''\n",
    "    Returns a list of edges [node1=str, node2=str]\n",
    "    '''\n",
    "    with open(edge_file) as opened_edge:\n",
    "        edge_file_lines = opened_edge.readlines()\n",
    "    \n",
    "    edge_list = []\n",
    "    for line in edge_file_lines:\n",
    "            split_line = line.split()\n",
    "            edge_list.append(split_line)\n",
    "            \n",
    "    return edge_list\n",
    "\n",
    "def read_featnames(featname_file):\n",
    "    '''\n",
    "    Returns a list of feature ID's which are strings\n",
    "    '''\n",
    "    with open(featname_file) as opened_featname:\n",
    "        featname_file_lines = opened_featname.readlines()\n",
    "\n",
    "    featname_list = []\n",
    "    for line in featname_file_lines:\n",
    "            split_line = line.split()  \n",
    "            featname_list.append(split_line[3].strip())\n",
    "            \n",
    "    return featname_list\n",
    "\n",
    "def get_num_feas(featname_dir):\n",
    "    full_featname_list = []\n",
    "    for file in os.listdir(featname_dir):\n",
    "        if file.endswith('.featnames'):\n",
    "            featnames = read_featnames(featname_dir+file)\n",
    "            full_featname_list += featnames\n",
    "        \n",
    "    return len(set(full_featname_list))\n",
    "\n",
    "def create_graph(data_dir, graph_type='normal'):\n",
    "    '''\n",
    "    creates a graph from the data files in datadir.\n",
    "    graph type options are 'normal', 'directed'\n",
    "    '''\n",
    "    if graph_type == 'normal':\n",
    "        graph = nx.Graph()\n",
    "    elif graph_type == 'directed':\n",
    "        graph = nx.DiGraph()\n",
    "    else:\n",
    "        print('invalid graph type')\n",
    "        exit()\n",
    "        \n",
    "    num_feas = get_num_feas(data_dir)\n",
    "\n",
    "    for file in os.listdir(data_dir):\n",
    "        if file.endswith('.feat'):\n",
    "            nodes = read_nodes(data_dir+file, num_feas)\n",
    "            graph.add_nodes_from(nodes)\n",
    "        elif file.endswith('.egofeat'):\n",
    "            nodes = read_ego_nodes(data_dir+file, num_feas)\n",
    "            graph.add_nodes_from(nodes)\n",
    "        elif file.endswith('combined.txt'):\n",
    "            edges = read_edges(data_dir+file)\n",
    "            graph.add_edges_from(edges)  \n",
    "            \n",
    "    return graph\n",
    "        \n",
    "def create_feature_array(graph):\n",
    "    '''\n",
    "    returns a feature array (nodes, node_features)\n",
    "    '''\n",
    "    num_features = len(list(graph.nodes(data=True))[0][1]['features'])\n",
    "    num_nodes = graph.number_of_nodes()\n",
    "    feature_array = np.zeros((num_nodes, num_features))\n",
    "    for i in range(num_nodes):\n",
    "        feature_array[i,:] = list(graph.nodes(data=True))[i][1]['features']\n",
    "        \n",
    "    return feature_array\n",
    "\n",
    "def create_feature_array_v2(graph):\n",
    "    '''\n",
    "    creates a feature array (nodes, node_features*2) which takes into account edge information by avergaing the\n",
    "    feature values of all neighbors and concatenating it with the central node feature vector\n",
    "    '''\n",
    "    num_features = len(list(graph.nodes(data=True))[0][1]['features'])\n",
    "    num_nodes = graph.number_of_nodes()\n",
    "    feature_array = np.zeros((num_nodes, num_features*2))\n",
    "    \n",
    "    for i in range(num_nodes):\n",
    "        center_node_features = list(graph.nodes(data=True))[i][1]['features']\n",
    "        neighbors = graph[list(graph.nodes(data=True))[i][0]]\n",
    "        sum_neighbor_features = np.zeros(num_features)\n",
    "        for neighbor in neighbors:\n",
    "            sum_neighbor_features += graph.node[neighbor]['features']\n",
    "        avg_neighbor_features = sum_neighbor_features/len(neighbors.keys())\n",
    "        feature_array[i,:] = np.concatenate((center_node_features, avg_neighbor_features))\n",
    "        \n",
    "    return feature_array\n",
    "\n",
    "def run_log_reg(feature_array, feature_index, test_split=0.1, regularization='l2'):\n",
    "    '''\n",
    "    feature_index = which feature to predict\n",
    "    test_split = what portion of data to use as a test set\n",
    "    regularization = l1', 'l2', or 'elasticnet'\n",
    "    '''\n",
    "    X = np.delete(feature_array, feature_index, axis=1)\n",
    "    y = feature_array[:, feature_index]\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_split)\n",
    "    log_reg = LogisticRegression(penalty = regularization).fit(X_train, y_train)\n",
    "    ypred = log_reg.predict(X_test)\n",
    "    pCC = stats.pearsonr(ypred, y_test)[0]\n",
    "    print('The Pearson correlation is: ', pCC)\n",
    "    \n",
    "fb_graph = create_graph('facebook_data/')\n",
    "print(nx.info(fb_graph))\n",
    "print(fb_graph['1'])\n",
    "fb_feature_array = create_feature_array_v2(fb_graph)\n",
    "run_log_reg(fb_feature_array, 2)\n",
    "\n",
    "# gplus_graph = create_graph('gplus/', 'directed')\n",
    "# print(nx.info(gplus_graph))\n",
    "# gplus_feature_array = create_feature_array(gplus_graph)\n",
    "# run_log_reg(gplus_feature_array, 2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.  0.  0. ... -1. -1. -1.]\n",
      "0.075\n"
     ]
    }
   ],
   "source": [
    "def get_visibility(graph, feature_index, awards, contendors):\n",
    "    '''\n",
    "    Returns the visibility, according to the formula #positive/#observed\n",
    "    feature_index = the index of the protected feature to check\n",
    "    awards = a list or array of length num_nodes which has a positive (1) or negative (0) assignment for each node, \n",
    "    based on whether or not that node was labeled successful or not\n",
    "    contendors = a list or array of length num_nodes which has a positive (1) or negative (0) assignment for each node, \n",
    "    based on whether or not that node was qualified or not\n",
    "    '''\n",
    "    num_positive = 0\n",
    "    observing_set = set()\n",
    "    for i in range(graph.number_of_nodes()):\n",
    "        if (awards[i] == contendors[i]) and (list(graph.nodes(data=True))[i][1]['features'][feature_index] == 1):\n",
    "            num_positive += 1\n",
    "            for neighbor in graph[list(graph.nodes(data=True))[i][0]]:\n",
    "                if graph.node[neighbor]['features'][feature_index] == 1:\n",
    "                    observing_set.add(neighbor)\n",
    "                \n",
    "    return num_positive/(num_positive + len(observing_set))\n",
    "    \n",
    "def get_visibility_v2(graph, feature_index, awards, contendors):\n",
    "    '''\n",
    "    Returns the visibility, according to the formula #positive/#observed\n",
    "    feature_index = the index of the protected feature to check\n",
    "    awards = a list or array of length num_nodes which has a positive (1) or negative (0) assignment for each node\n",
    "    contendors = a list or array of length num_nodes which has a positive (1) or negative (0) assignment for each node, \n",
    "    based on whether or not that node was qualified or not\n",
    "    '''\n",
    "    num_false_negative = 0\n",
    "    observing_set = set()\n",
    "    for i in range(graph.number_of_nodes()):\n",
    "        if (awards[i] == 0 and contendors[i] == 1) \\\n",
    "        and (list(graph.nodes(data=True))[i][1]['features'][feature_index] == 1):\n",
    "            num_false_negative += 1\n",
    "            for neighbor in graph[list(graph.nodes(data=True))[i][0]]:\n",
    "                if graph.node[neighbor]['features'][feature_index] == 1:\n",
    "                    observing_set.add(neighbor)\n",
    "                \n",
    "    return num_false_negative/(num_false_negative + len(observing_set))    \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
